services:
  triton-server:
    image: nvcr.io/nvidia/tritonserver:22.06-py3
    command: tritonserver --model-repository=/models --strict-model-config=false --log-verbose 1
    ports:
      - "8000:8000"  # HTTP endpoint
      - "8001:8001"  # gRPC endpoint
      - "8002:8002"  # Metrics endpoint
    volumes:
      - ./triton-deploy/models:/models
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    # shm_size: 1g
    # ulimits:
    #   memlock: -1
    #   stack: 67108864

  yolo_inference:
    build:
      context: ../../..
      dockerfile: Src/app/yolo_deploy/scheduler/Dockerfile
    container_name: yolo_inference_cont # Assign a speciyolo_inference_contic container name
    restart: unless-stopped # Example restart policy
    volumes:
      - ../../../input:/app/input
      - ../../../output:/app/output
    depends_on:
      - fastapi-app